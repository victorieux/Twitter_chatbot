{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/Currie32/Chatbot-from-Movie-Dialogue/blob/master/Chatbot_Attention.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import csv \n",
    "import string \n",
    "import re\n",
    "import tensorflow as tf\n",
    "tweetdata = pd.read_csv(\"tweets.csv\", sep='delimiter', engine='python')\n",
    "retweetdata = pd.read_csv(\"retweets.csv\", sep='delimiter', engine='python')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of the tweets and retweets\n",
    "df = pd.DataFrame(tweetdata)\n",
    "df1 = pd.DataFrame(retweetdata)\n",
    "tweets = df[\"Tweets\"].tolist()\n",
    "retweets = df1[\"Retweets\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something resembling hell': how does the rest of the world view the UK? https://t.co/mobKGxKBkH https://t.co/0AUuM3syKm\n",
      "Communities Secretary @RobertJenrick has today announced £20m for councils to prepare for #Brexit and asked them each to appoint a Brexit lead to work with central government.\n",
      "\n",
      "Don't agree with his #brexit view, but @DaveRowntree on @BBC6Music was a good listen. #festivalofthemoon #blur @StuartMaconie\n",
      "\"Jeremy Corbyn today:\n",
      "\n",
      "@DmitryOpines @TheScepticIsle But I believe I will fly #brexit\n",
      "'>If Johnson tries to force through No Deal we will put down a motion for a referendum on No Deal vs Remain, and will back for remain<\n",
      "\n",
      "\"Boris to employ 'Benny Hill Strategy' in Brexit negotiations\n",
      "#brexit #LabourBrexitPolicy #Remain #ToryMess @jeremycorbyn  \"\n",
      "\n",
      "This morning, Downing Street has announced that Mr Johnson intends to utilise a 'Benny Hill Strategy' during the next round of Brexit negotiations in Brussels.  #Boris #BorisJohnson #brexit #  \"\n",
      "\"Pro leave voter sends DEATH THREATS to pro remain politicians. @brexitparty_uk #brexit @Nigel_Farage #nigelfarage @Daily_Express #leave #brexiteer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(tweets[i])\n",
    "    print(retweets[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23403\n",
      "32697\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of tweets and retweets\n",
    "print(len(tweets))\n",
    "print(len(retweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.\n",
    "        Also removing hashtags and @users\n",
    "    '''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_tweets = []\n",
    "for tweet in tweets:\n",
    "    clean_tweets.append(clean_text(tweet))\n",
    "    \n",
    "clean_retweets = []    \n",
    "for retweet in retweets:\n",
    "    clean_retweets.append(clean_text(retweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for tweets in clean_tweets:\n",
    "    lengths.append(len(tweet.split()))\n",
    "for retweet in clean_retweets:\n",
    "    lengths.append(len(retweet.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.331569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.863651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             counts\n",
       "count  56100.000000\n",
       "mean      17.331569\n",
       "std       10.863651\n",
       "min        0.000000\n",
       "25%        8.000000\n",
       "50%       21.000000\n",
       "75%       21.000000\n",
       "max       99.000000"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.???\n",
    "min_line_length = 1\n",
    "max_line_length = 60\n",
    "\n",
    "# Filter out the tweets that are too short/long\n",
    "short_tweets_temp = []\n",
    "short_retweets_temp = []\n",
    "\n",
    "i = 0\n",
    "for tweet in clean_tweets:\n",
    "    if len(tweet.split()) >= min_line_length and len(tweet.split()) <= max_line_length:\n",
    "        short_tweets_temp.append(tweet)\n",
    "        short_retweets_temp.append(clean_retweets[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the retweets that are too short/long\n",
    "short_tweets = []\n",
    "short_retweets = []\n",
    "\n",
    "i = 0\n",
    "for retweet in short_retweets_temp:\n",
    "    if len(retweet.split()) >= min_line_length and len(retweet.split()) <= max_line_length:\n",
    "        short_retweets.append(retweet)\n",
    "        short_tweets.append(short_tweets_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tweets: 20166\n",
      "# of retweets: 20166\n",
      "% of data used: 61.68%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of tweets:\", len(short_tweets))\n",
    "print(\"# of retweets:\", len(short_retweets))\n",
    "print(\"% of data used: {}%\".format(round(len(short_retweets)/len(retweets),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for tweet in short_tweets:\n",
    "    for word in tweet.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for retweet in short_retweets:\n",
    "    for word in retweet.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 5\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 28576\n",
      "Size of vocab we will use: 7856\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "tweets_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        tweets_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "retweets_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        retweets_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    tweets_vocab_to_int[code] = len(tweets_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    retweets_vocab_to_int[code] = len(retweets_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "tweets_int_to_vocab = {v_i: v for v, v_i in tweets_vocab_to_int.items()}\n",
    "retweets_int_to_vocab = {v_i: v for v, v_i in retweets_vocab_to_int.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7860\n",
      "7860\n",
      "7860\n",
      "7860\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dictionaries.\n",
    "print(len(tweets_vocab_to_int))\n",
    "print(len(tweets_int_to_vocab))\n",
    "print(len(retweets_vocab_to_int))\n",
    "print(len(retweets_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every retweet.\n",
    "for i in range(len(short_retweets)):\n",
    "    short_retweets[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "tweets_int = []\n",
    "for tweet in short_tweets:\n",
    "    ints = []\n",
    "    for word in tweet.split():\n",
    "        if word not in tweets_vocab_to_int:\n",
    "            ints.append(tweets_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(tweets_vocab_to_int[word])\n",
    "    tweets_int.append(ints)\n",
    "    \n",
    "retweets_int = []\n",
    "for retweet in short_retweets:\n",
    "    ints = []\n",
    "    for word in retweet.split():\n",
    "        if word not in retweets_vocab_to_int:\n",
    "            ints.append(retweets_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(retweets_vocab_to_int[word])\n",
    "    retweets_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20166\n",
      "20166\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(tweets_int))\n",
    "print(len(retweets_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 799535\n",
      "Number of times <UNK> is used: 37483\n",
      "Percent of words that are <UNK>: 4.69%\n"
     ]
    }
   ],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for tweet in tweets_int:\n",
    "    for word in tweet:\n",
    "        if word == tweets_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for retweet in retweets_int:\n",
    "    for word in retweet:\n",
    "        if word == retweets_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20166\n",
      "20166\n",
      "\n",
      "[4037]\n",
      "[5142, 7858, 7858, 7858, 7858, 7858, 7858, 7858]\n",
      "\n",
      "[3979]\n",
      "[440, 6810, 1763, 7858, 7858, 7858, 7858, 7858, 7858, 7858]\n",
      "\n",
      "[4169]\n",
      "[912, 7227, 6810, 2374, 1069, 1926, 5789, 4479, 4590, 1922, 6147, 4085, 871, 1489, 5936, 5440, 7858, 7858, 7858, 7858, 7858, 7858, 7858]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort tweets and retweets by the length of tweets.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "sorted_tweets = []\n",
    "sorted_retweets = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(tweets_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_tweets.append(tweets_int[i[0]])\n",
    "            sorted_retweets.append(retweets_int[i[0]])\n",
    "\n",
    "print(len(sorted_tweets))\n",
    "print(len(sorted_retweets))\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(sorted_tweets[i])\n",
    "    print(sorted_retweets[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \n",
    "    '''Create palceholders for inputs to the model.  A placeholder is simply a variable that we will assign data to at a later date. \n",
    "        It allows us to create our operations and build our computation graph, without needing the data. \n",
    "        Bassicly a place in memory where we will store value later on\n",
    "        the arguments that it takes is the dtype of the data which will be put in, the shape of the tensor and the name for the operation\n",
    "        \n",
    "        Questions for Ben : Why do we specify the shape as [None, None] when having it blank will have as default a None shape?\n",
    "        \n",
    "        \n",
    "        Answer: inputs placeholder will be fed with English sentence data, and its shape is [None, None]. The first None means the batch size, \n",
    "        and the batch size is unknown since user can set it. The second None means the lengths of sentences. \n",
    "        The maximum length of setence is different from batch to batch, so it cannot be set with the exact number.\n",
    "        '''\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the beginning of each batch\n",
    "        tf.strided_slice() extracts a strided slice of a tensor.it will remove the final word from each batch\n",
    "        tf.concat concatenates tensors along one dimension. tf.fill creates a tensor with a certain shape and fill it with \n",
    "        values. So here we have tensor with the shape batchsize and 1 witht he values vocab_to_int with \"GO\" as ending\n",
    "        Benjamin: Explain the second line of code'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    \n",
    "    '''Create the encoding layer\n",
    "    tf.contrib contains contributed code. In this code we will use the rnn \n",
    "    which contains RNN Cells and additional RNN operations. BasicLSTMCell is for Basic LSTM recurrent network cell.\n",
    "    the argument which will be passed in is the rnn_size.\n",
    "    \n",
    "    the DropoutWrapper adds dropout to inputs and outputs of the given cell. the arguments being firslty: input_keep_prob which\n",
    "    is a unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added.\n",
    "    Second one cells which we put lstm as they outperform GCU's.\n",
    "    \n",
    "    Thirdly we pass a RNN cell composed sequentially of multiple simple cells. The first arguments are the cells.\n",
    "    This being the drop from before times the number of layers we'll pass in\n",
    "    \n",
    "    Fourthly, making the encoder bidirectional proved to be much more effective than a simple feed forward network.\n",
    "    the arguments which it takes is cell_fw which is an instance of RNNCell, to be used for forward direction.\n",
    "    the second argument being cell_bw which is an instance of RNNCell, to be used for backward direction.\n",
    "    the sequence_lenght which is containing the actual lengths for each of the sequences in the batch. If not \n",
    "    provided, all batch entries are assumed to be full sequences.\n",
    "    the inputs which are the RNN inputs. Lastly the dtype which is The data type for the initial states and expected output.\n",
    "    \n",
    "    We return only the encoder’s state because it is the input for our decoding layer. \n",
    "    Simply put, the weights of the encoding cells are what interest us.\n",
    "    Why ltsm with basic vell and the end_cell with multi?\n",
    "    '''\n",
    "\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    '''Decode the training data\n",
    "    firstly, we create a a tensor with all elements set to zero.\n",
    "    The model performs best when the attention states are set with zeros. \n",
    "    '''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ising\\Anaconda3Victor\\envs\\py354\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ising\\Anaconda3Victor\\envs\\py354\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ising\\Anaconda3Victor\\envs\\py354\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ising\\Anaconda3Victor\\envs\\py354\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ising\\Anaconda3Victor\\envs\\py354\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.reset_default_graph()\n",
    "# Start the session\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(retweets_vocab_to_int), \n",
    "    len(tweets_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    tweets_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def batch_data(tweets, retweets, batch_size):\n",
    "    \"\"\"Batch tweets and retweets together\"\"\"\n",
    "    for batch_i in range(0, len(tweets)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        tweets_batch = tweets[start_i:start_i + batch_size]\n",
    "        retweets_batch = retweets[start_i:start_i + batch_size]\n",
    "        pad_tweets_batch = np.array(pad_sentence_batch(tweets_batch, tweets_vocab_to_int))\n",
    "        pad_retweets_batch = np.array(pad_sentence_batch(retweets_batch, retweets_vocab_to_int))\n",
    "        yield pad_tweets_batch, pad_retweets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10086\n",
      "1779\n"
     ]
    }
   ],
   "source": [
    "# Validate the training with 10% of the data\n",
    "train_valid_split = int(len(sorted_tweets)*0.15)\n",
    "\n",
    "# Split the tweets and retweets into training and validating data\n",
    "train_tweets = sorted_tweets[train_valid_split:]\n",
    "train_retweets = sorted_retweets[train_valid_split:]\n",
    "\n",
    "valid_tweets = sorted_tweets[:train_valid_split]\n",
    "valid_retweets = sorted_retweets[:train_valid_split]\n",
    "\n",
    "print(len(train_tweets))\n",
    "print(len(valid_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "display_step = 100 # Check training loss after every 100 batches\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_tweets))//batch_size//2)-1 # Modulus for checking validation loss\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch_i in range(1, epochs+1):\n",
    "    for batch_i, (tweets_batch, retweets_batch) in enumerate(\n",
    "            batch_data(train_tweets, train_retweets, batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: tweets_batch,\n",
    "             targets: retweets_batch,\n",
    "             lr: learning_rate,\n",
    "             sequence_length: retweets_batch.shape[1],\n",
    "             keep_prob: keep_probability})\n",
    "\n",
    "        total_train_loss += loss\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        \n",
    "        if batch_i % display_step == 0:\n",
    "            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                  .format(epoch_i,\n",
    "                          epochs, \n",
    "                          batch_i, \n",
    "                          len(train_tweets) // batch_size, \n",
    "                          total_train_loss / display_step, \n",
    "                          batch_time*display_step))\n",
    "            total_train_loss = 0\n",
    "\n",
    "        if batch_i % validation_check == 0 and batch_i > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_time = time.time()\n",
    "            for batch_ii, (tweets_batch, retweets_batch) in \\\n",
    "                    enumerate(batch_data(valid_tweets, valid_retweets, batch_size)):\n",
    "                valid_loss = sess.run(\n",
    "                cost, {input_data: tweets_batch,\n",
    "                       targets: retweets_batch,\n",
    "                       lr: learning_rate,\n",
    "                       sequence_length: retweets_batch.shape[1],\n",
    "                       keep_prob: 1})\n",
    "                total_valid_loss += valid_loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            avg_valid_loss = total_valid_loss / (len(valid_tweets) / batch_size)\n",
    "            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "            \n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            summary_valid_loss.append(avg_valid_loss)\n",
    "            if avg_valid_loss <= min(summary_valid_loss):\n",
    "                print('New Record!') \n",
    "                stop_early = 0\n",
    "                saver = tf.train.Saver() \n",
    "                saver.save(sess, checkpoint)\n",
    "\n",
    "            else:\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == stop:\n",
    "                    break\n",
    "    \n",
    "    if stop_early == stop:\n",
    "        print(\"Stopping Training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_seq(tweet, vocab_to_int):\n",
    "    \n",
    "    '''Prepare the tweet for the model'''\n",
    "    \n",
    "    tweet = clean_text(tweet)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in tweet.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own input question\n",
    "#input_question = 'How are you?'\n",
    "\n",
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(short_tweets))\n",
    "input_tweet = short_tweets[random]\n",
    "\n",
    "#input own question\n",
    "#input_question = \"Do you like Theresa May?\"\n",
    "#input_tweet = tweet_to_seq(input_question, tweets_vocab_to_int)\n",
    "\n",
    "# Prepare the question\n",
    "input_tweet = tweet_to_seq(input_tweet, tweets_vocab_to_int)\n",
    "\n",
    "# Pad the questions until it equals the max_line_length\n",
    "input_tweet = input_tweet + [tweets_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_tweet))\n",
    "# Add empty questions so the the input_data is the correct shape\n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "# Set the first question to be out input question\n",
    "batch_shell[0] = input_tweet    \n",
    "    \n",
    "# Run the model with the input question\n",
    "retweet_logits = sess.run(inference_logits, {input_data: batch_shell, \n",
    "                                            keep_prob: 1.0})[0]\n",
    "\n",
    "# Remove the padding from the Question and Answer\n",
    "pad_q = tweets_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = retweets_vocab_to_int[\"<PAD>\"]\n",
    "\n",
    "print('Question')\n",
    "print('  Word Ids:      {}'.format([i for i in input_tweet if i != pad_q]))\n",
    "print('  Input Words: {}'.format([tweets_int_to_vocab[i] for i in input_tweet if i != pad_q]))\n",
    "\n",
    "print('\\nAnswer')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(retweet_logits, 1) if i != pad_a]))\n",
    "print('  Response Words: {}'.format([retweets_int_to_vocab[i] for i in np.argmax(retweet_logits, 1) if i != pad_a]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpy354",
   "language": "python",
   "name": "mpy354"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
